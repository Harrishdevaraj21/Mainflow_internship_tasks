{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f799c27"
      },
      "source": [
        "# Task\n",
        "Create a time series forecasting model using the data from \"/content/sales_data.csv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55f94fd"
      },
      "source": [
        "## Load Sales Data\n",
        "\n",
        "### Subtask:\n",
        "Load the sales data from the provided source. Ensure that the dataset includes 'Date' and 'Sales' columns. If a file is not explicitly provided, I will assume a common CSV format and prompt for a filename if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "199485fd"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to import the pandas library, which is essential for data manipulation in Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a958c85a",
        "outputId": "8d4d2e2c-c5f5-4126-915c-8430b1ece4e4"
      },
      "source": [
        "import pandas as pd\n",
        "print(\"Pandas library imported successfully.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas library imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ade5b74"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to load the sales data from the CSV file '/content/sales_data.csv' into a pandas DataFrame, parsing the 'Date' column as datetime objects as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "8794f379",
        "outputId": "d402634f-a46a-4c35-ba80-434d3d4b6a14"
      },
      "source": [
        "df = pd.read_csv('/content/sales_data.csv', parse_dates=['Date'])\n",
        "print(\"Sales data loaded successfully.\")\n",
        "print(df.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing column provided to 'parse_dates': 'Date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2200207753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sales_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sales data loaded successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_parse_dates_presence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_noconvert_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_validate_parse_dates_presence\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m    241\u001b[0m         )\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0;34mf\"Missing column provided to 'parse_dates': '{missing_cols}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8edd0342"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'parse_dates' argument in `pd.read_csv` could not find the 'Date' column during the initial parsing phase, despite it being present in the data. To fix this, I will first load the CSV without attempting to parse dates, and then explicitly convert the 'Date' column to datetime objects using `pd.to_datetime` in a separate step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "bbd67329",
        "outputId": "8c8f0b6b-e214-4c15-ff9c-10d59a916003"
      },
      "source": [
        "df = pd.read_csv('/content/sales_data.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(\"Sales data loaded and 'Date' column parsed successfully.\")\n",
        "print(df.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1629007559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sales_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sales data loaded and 'Date' column parsed successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c464731"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a KeyError because the column name for the date information is 'date' (lowercase 'd') instead of 'Date' (uppercase 'D') in the loaded DataFrame. I need to correct the column name to 'date' when accessing it for conversion to datetime objects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ae8076",
        "outputId": "82c97a5f-0d0f-4d5f-b290-9f98ac3cba74"
      },
      "source": [
        "df = pd.read_csv('/content/sales_data.csv')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "print(\"Sales data loaded and 'date' column parsed successfully.\")\n",
        "print(df.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales data loaded and 'date' column parsed successfully.\n",
            "        date       Sales\n",
            "0 2020-01-01  104.967142\n",
            "1 2020-02-01  126.474500\n",
            "2 2020-03-01  155.492441\n",
            "3 2020-04-01  173.801727\n",
            "4 2020-05-01  152.388308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e07fa88"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly requires the column to be named 'Date' (uppercase 'D'), but the current DataFrame has a column named 'date' (lowercase 'd'). To fulfill the subtask's requirement, I need to rename the 'date' column to 'Date'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d728100",
        "outputId": "216bdbb9-87ed-486e-de55-532c5c939f77"
      },
      "source": [
        "df = df.rename(columns={'date': 'Date'})\n",
        "print(\"Column 'date' renamed to 'Date' successfully.\")\n",
        "print(df.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'date' renamed to 'Date' successfully.\n",
            "        Date       Sales\n",
            "0 2020-01-01  104.967142\n",
            "1 2020-02-01  126.474500\n",
            "2 2020-03-01  155.492441\n",
            "3 2020-04-01  173.801727\n",
            "4 2020-05-01  152.388308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08c8e51b"
      },
      "source": [
        "## Preprocess Data\n",
        "\n",
        "### Subtask:\n",
        "Convert the 'Date' column to a datetime format and set it as the DataFrame index. Handle any missing values or irregularities in the 'Sales' column to prepare the data for time series analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086c872b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to set the 'Date' column as the DataFrame index. This is a crucial step for time series analysis as it allows for time-based operations and simplifies data handling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cfc1cf",
        "outputId": "8134838c-4129-465d-bca9-5a1c2b28adec"
      },
      "source": [
        "df.set_index('Date', inplace=True)\n",
        "print(\"Date column set as DataFrame index.\")\n",
        "print(df.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date column set as DataFrame index.\n",
            "                 Sales\n",
            "Date                  \n",
            "2020-01-01  104.967142\n",
            "2020-02-01  126.474500\n",
            "2020-03-01  155.492441\n",
            "2020-04-01  173.801727\n",
            "2020-05-01  152.388308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4eb05a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the 'Date' column is set as the index, the next step is to check for any missing values in the DataFrame, as specified in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0054e52",
        "outputId": "c2dc4bc5-9013-462a-dce9-de183abf2a78"
      },
      "source": [
        "print(\"Checking for missing values:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values:\n",
            "Sales    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddce54b"
      },
      "source": [
        "## Visualize Sales Trends\n",
        "\n",
        "### Subtask:\n",
        "Plot the sales data over time using a line chart. Include a moving average to highlight trends and identify any seasonality or irregularities in the sales patterns. Ensure the plot has appropriate labels and legends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0732385"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to import the `matplotlib.pyplot` module as `plt`, which is necessary for creating visualizations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93363a06",
        "outputId": "e93a210f-9203-4737-cc0f-6f29fe1ddb25"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"Matplotlib imported successfully.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matplotlib imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54472ea4"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to calculate the 12-month rolling mean of the 'Sales' column, as specified in the instructions, which will help highlight trends in the sales data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3633358",
        "outputId": "4e2841f5-f735-4745-c7b7-fee3727ba865"
      },
      "source": [
        "df['Moving Average'] = df['Sales'].rolling(window=12).mean()\n",
        "print(\"12-month moving average calculated and added to DataFrame.\")\n",
        "print(df.head(15))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12-month moving average calculated and added to DataFrame.\n",
            "                 Sales  Moving Average\n",
            "Date                                  \n",
            "2020-01-01  104.967142             NaN\n",
            "2020-02-01  126.474500             NaN\n",
            "2020-03-01  155.492441             NaN\n",
            "2020-04-01  173.801727             NaN\n",
            "2020-05-01  152.388308             NaN\n",
            "2020-06-01  136.944345             NaN\n",
            "2020-07-01  132.934985             NaN\n",
            "2020-08-01  102.674347             NaN\n",
            "2020-09-01   74.861129             NaN\n",
            "2020-10-01   81.139886             NaN\n",
            "2020-11-01   80.635981             NaN\n",
            "2020-12-01  101.771274      118.673839\n",
            "2021-01-01  136.705337      121.318688\n",
            "2021-02-01  143.010055      122.696651\n",
            "2021-03-01  166.052092      123.576622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55e5259"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, I need to create a figure and an axes object using `matplotlib.pyplot` to prepare for plotting the sales data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "2f247c3c",
        "outputId": "dd0d797f-a575-4e7b-80fb-77f133d4941a"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "print(\"Figure and axes object created.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure and axes object created.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae498d56"
      },
      "source": [
        "**Reasoning**:\n",
        "As specified in the subtask, the next step is to plot the 'Sales' data from the DataFrame `df` on the previously created axes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6ff5059",
        "outputId": "1b42d590-3b6e-40b6-c50f-5bc21f30ca43"
      },
      "source": [
        "ax.plot(df.index, df['Sales'], label='Sales', color='blue')\n",
        "print(\"Sales data plotted.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales data plotted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfd2970"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to plot the 'Moving Average' data from the DataFrame `df` on the same axes to visualize the trend, as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b12fd5",
        "outputId": "91552ed3-4ed0-4cb7-a7da-37941a2695f8"
      },
      "source": [
        "ax.plot(df.index, df['Moving Average'], label='Moving Average', color='red', linestyle='--')\n",
        "print(\"Moving average data plotted.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving average data plotted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a274176"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to add a title to the plot to clearly describe its content, as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2dff97a",
        "outputId": "cc5ffe5e-9ae6-4867-d245-a090dbf60fde"
      },
      "source": [
        "ax.set_title('Sales Data with 12-Month Moving Average')\n",
        "print(\"Plot title set.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot title set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efd4a777"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to label the x-axis as 'Date' and the y-axis as 'Sales', as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "877d1e90",
        "outputId": "3f3d91ac-c2f3-4fd6-ce31-486f8a6cbea8"
      },
      "source": [
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Sales')\n",
        "print(\"X-axis and Y-axis labels set.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X-axis and Y-axis labels set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4be72721"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to add a legend to differentiate between the 'Sales' and 'Moving Average' lines, as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fbd0daa",
        "outputId": "7ed4d2fb-5223-460f-9fca-d8a7fd3e775e"
      },
      "source": [
        "ax.legend()\n",
        "print(\"Legend added to the plot.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Legend added to the plot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d133e11"
      },
      "source": [
        "**Reasoning**:\n",
        "The final instruction is to display the plot, which will render the visualization with the sales data, moving average, title, labels, and legend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "731d9bfb",
        "outputId": "2ef320f6-03bb-47b2-84b5-fbfeef7ab523"
      },
      "source": [
        "plt.show()\n",
        "print(\"Plot displayed successfully.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot displayed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c29d7ca2"
      },
      "source": [
        "## Apply ARIMA for Forecasting\n",
        "\n",
        "### Subtask:\n",
        "Apply the AutoRegressive Integrated Moving Average (ARIMA) model to the preprocessed sales data. Train the model and generate sales forecasts for future periods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a8da5c4"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in applying the ARIMA model is to import the necessary class, `ARIMA`, from `statsmodels.tsa.arima.model`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c862e9ee",
        "outputId": "b03d6ad5-578a-4cdf-8999-1cb5339f3b93"
      },
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "print(\"ARIMA model imported successfully.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA model imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b9ef92f"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the order of the ARIMA model, which includes the autoregressive (p), integrated (d), and moving average (q) components. A common starting point is (5,1,0) as suggested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "334f55de",
        "outputId": "01985bf5-b6c0-436c-f50b-e5dd36a12ae5"
      },
      "source": [
        "arima_order = (5, 1, 0)\n",
        "print(f\"ARIMA order defined as: {arima_order}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA order defined as: (5, 1, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f046c4c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to instantiate the ARIMA model by passing the 'Sales' data from the DataFrame `df` and the previously defined `arima_order`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eef7a92b",
        "outputId": "3028a497-ac83-4802-b41b-a114459317e3"
      },
      "source": [
        "model = ARIMA(df['Sales'], order=arima_order)\n",
        "print(\"ARIMA model instantiated successfully.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA model instantiated successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77d86c0a"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step, as specified in the subtask instructions, is to fit the instantiated ARIMA model to the data to train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ab9ca00",
        "outputId": "8b11d626-031b-4fbb-eaf4-da0f456cafe4"
      },
      "source": [
        "model_fit = model.fit()\n",
        "print(\"ARIMA model fitted successfully.\")\n",
        "print(model_fit.summary())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA model fitted successfully.\n",
            "                               SARIMAX Results                                \n",
            "==============================================================================\n",
            "Dep. Variable:                  Sales   No. Observations:                   36\n",
            "Model:                 ARIMA(5, 1, 0)   Log Likelihood                -147.386\n",
            "Date:                Fri, 16 Jan 2026   AIC                            306.771\n",
            "Time:                        05:29:52   BIC                            316.103\n",
            "Sample:                    01-01-2020   HQIC                           309.993\n",
            "                         - 12-01-2022                                         \n",
            "Covariance Type:                  opg                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "ar.L1          0.1295      0.191      0.679      0.497      -0.244       0.503\n",
            "ar.L2          0.2538      0.185      1.373      0.170      -0.109       0.616\n",
            "ar.L3          0.2406      0.191      1.259      0.208      -0.134       0.615\n",
            "ar.L4         -0.4106      0.162     -2.536      0.011      -0.728      -0.093\n",
            "ar.L5         -0.3840      0.175     -2.190      0.029      -0.728      -0.040\n",
            "sigma2       248.7853     88.069      2.825      0.005      76.172     421.398\n",
            "===================================================================================\n",
            "Ljung-Box (L1) (Q):                   1.60   Jarque-Bera (JB):                 1.07\n",
            "Prob(Q):                              0.21   Prob(JB):                         0.59\n",
            "Heteroskedasticity (H):               1.81   Skew:                            -0.02\n",
            "Prob(H) (two-sided):                  0.32   Kurtosis:                         2.14\n",
            "===================================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b9b26f"
      },
      "source": [
        "**Reasoning**:\n",
        "The final step in the subtask is to generate forecasts for a future period, specifically the next 12 months, using the fitted ARIMA model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "864686b4",
        "outputId": "b7cad8cc-a3e3-431e-d7f9-2a1dcd739581"
      },
      "source": [
        "forecast_steps = 12\n",
        "forecast = model_fit.predict(start=len(df), end=len(df) + forecast_steps - 1)\n",
        "print(f\"Generated forecasts for the next {forecast_steps} months.\")\n",
        "print(forecast.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated forecasts for the next 12 months.\n",
            "2023-01-01    179.727820\n",
            "2023-02-01    207.886737\n",
            "2023-03-01    209.849183\n",
            "2023-04-01    210.137398\n",
            "2023-05-01    210.214807\n",
            "Freq: MS, Name: predicted_mean, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd223c2b"
      },
      "source": [
        "## Validate ARIMA Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the accuracy of the ARIMA model's forecasts using metrics such as Root Mean Squared Error (RMSE) or Mean Absolute Percentage Error (MAPE). This step will provide insights into the model's predictive performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63103b26"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in evaluating the ARIMA model is to split the data into training and testing sets, using the `forecast_steps` variable to determine the split point, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf5fe679",
        "outputId": "13694719-13f5-4c9f-8ca8-fea3f7268ccf"
      },
      "source": [
        "train_data = df['Sales'][:-forecast_steps]\n",
        "test_data = df['Sales'][-forecast_steps:]\n",
        "print(f\"Training data length: {len(train_data)}\")\n",
        "print(f\"Testing data length: {len(test_data)}\")\n",
        "print(\"Data split into training and testing sets successfully.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 24\n",
            "Testing data length: 12\n",
            "Data split into training and testing sets successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b58240"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to instantiate and fit a new ARIMA model on the `train_data` using the `arima_order` defined previously, as specified in the instructions for evaluating the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59c32d9d",
        "outputId": "616abac4-6433-40bf-cee3-95756efd00b9"
      },
      "source": [
        "model_eval = ARIMA(train_data, order=arima_order)\n",
        "model_eval_fit = model_eval.fit()\n",
        "print(\"ARIMA model instantiated and fitted on training data successfully.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA model instantiated and fitted on training data successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54e85c96"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to generate predictions on the `test_data` using the fitted ARIMA model (`model_eval_fit`), ensuring the predictions cover the exact period of the test set by setting the `start` and `end` parameters correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd47ea48",
        "outputId": "1e333d38-022f-4f4f-aa00-3667557a4f52"
      },
      "source": [
        "predictions = model_eval_fit.predict(start=len(train_data), end=len(df) - 1)\n",
        "print(\"Predictions generated for the test set.\")\n",
        "print(predictions.head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions generated for the test set.\n",
            "2022-01-01    114.801255\n",
            "2022-02-01    128.426208\n",
            "2022-03-01    127.044575\n",
            "2022-04-01    119.127293\n",
            "2022-05-01    128.076413\n",
            "Freq: MS, Name: predicted_mean, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a88db8f"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to import the `mean_squared_error` function from `sklearn.metrics`, which is necessary for calculating the RMSE to evaluate the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fa1485e",
        "outputId": "ba4b3e19-0266-4537-db3d-a66abdf83624"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"mean_squared_error imported successfully.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_squared_error imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dcdee3c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to calculate the Root Mean Squared Error (RMSE) using the `mean_squared_error` function and `numpy.sqrt` to evaluate the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0e8d51e",
        "outputId": "89b4a368-8057-4db9-e26f-5bdd073c9e37"
      },
      "source": [
        "import numpy as np\n",
        "rmse = np.sqrt(mean_squared_error(test_data, predictions))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 65.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dde2911"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the sales trends observed, present the sales forecasts generated by the ARIMA model, and discuss the model's accuracy and any key insights from the time series analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1954b9f0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Loading and Preprocessing**: The sales data from `/content/sales_data.csv` was successfully loaded. The 'date' column was correctly parsed as datetime objects, renamed to 'Date', and set as the DataFrame's index. A check for missing values confirmed no missing entries in the 'Sales' column, indicating clean data for analysis.\n",
        "*   **Sales Trend Visualization**: A 12-month moving average was calculated and plotted alongside the sales data. This visualization helped in observing underlying sales trends and potential seasonality.\n",
        "*   **ARIMA Model Application**: An ARIMA model with an order of (5, 1, 0) was successfully applied to the preprocessed sales data. This model was trained and used to generate sales forecasts for the subsequent 12 periods.\n",
        "*   **Model Validation**: The ARIMA model's predictive performance was evaluated by splitting the data into a training set (24 data points) and a test set (12 data points). The Root Mean Squared Error (RMSE) between the actual test data and the model's predictions was calculated to be 65.99.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The RMSE of 65.99 provides a quantitative measure of the model's accuracy. Further analysis could involve comparing this RMSE against the typical sales values to understand its relative performance or establishing a baseline RMSE from a simpler model for contextual comparison.\n",
        "*   To potentially improve the model's accuracy, consider experimenting with different ARIMA orders (p, d, q), incorporating external factors (exogenous variables) that might influence sales, or exploring other time series models such as SARIMA to better capture seasonality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a368c1d"
      },
      "source": [
        "# Task\n",
        "Create a logistic regression model to predict heart disease using the \"heart_disease.csv\" dataset. This involves loading and cleaning the data, normalizing numerical features, training the model, and evaluating its performance using accuracy, a confusion matrix, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55bffdf"
      },
      "source": [
        "## Load and Clean Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the 'heart_disease.csv' dataset into a pandas DataFrame. Check for missing or inconsistent data and clean the dataset by handling null values and duplicates. Display initial dataset information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52e3af7"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to import the pandas library, which is essential for data manipulation in Python, although it might have been imported previously, re-importing ensures the environment is ready for the new subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fed5f79",
        "outputId": "74912f61-c504-4cd0-87b0-0fdb103ba724"
      },
      "source": [
        "import pandas as pd\n",
        "print(\"Pandas library imported successfully.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas library imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d205ffef"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to load the 'heart_disease.csv' file into a DataFrame named `df` using `pd.read_csv()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4644eabc",
        "outputId": "76399cab-e7f5-407b-c4fe-c054aab2e7b5"
      },
      "source": [
        "df = pd.read_csv('/content/heart.csv')\n",
        "print(\"Heart disease data loaded successfully.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heart disease data loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f533987"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is loaded, the next instruction is to display the first 5 rows of the DataFrame using `df.head()` to get a quick overview of its structure and content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219fd9de",
        "outputId": "e6437f3b-5145-4507-d1d0-f7db7fdaf034"
      },
      "source": [
        "print(\"Displaying the first 5 rows of the DataFrame:\")\n",
        "print(df.head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying the first 5 rows of the DataFrame:\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
            "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
            "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
            "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
            "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   0     1       1  \n",
            "1   0     2       1  \n",
            "2   0     2       1  \n",
            "3   0     2       1  \n",
            "4   0     2       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098c476a"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to display a concise summary of the DataFrame, including data types and non-null values, using `df.info()` to understand the data types and completeness of each column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e67355ef",
        "outputId": "f161b053-35c6-433d-8f52-904f4e25dc8c"
      },
      "source": [
        "print(\"Displaying DataFrame information:\")\n",
        "df.info()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying DataFrame information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       303 non-null    int64  \n",
            " 1   sex       303 non-null    int64  \n",
            " 2   cp        303 non-null    int64  \n",
            " 3   trestbps  303 non-null    int64  \n",
            " 4   chol      303 non-null    int64  \n",
            " 5   fbs       303 non-null    int64  \n",
            " 6   restecg   303 non-null    int64  \n",
            " 7   thalach   303 non-null    int64  \n",
            " 8   exang     303 non-null    int64  \n",
            " 9   oldpeak   303 non-null    float64\n",
            " 10  slope     303 non-null    int64  \n",
            " 11  ca        303 non-null    int64  \n",
            " 12  thal      303 non-null    int64  \n",
            " 13  target    303 non-null    int64  \n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 33.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d74d4813"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to check for missing values in each column and display the count using `df.isnull().sum()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c9b0a9",
        "outputId": "3ce3555f-ebad-4f50-d359-23cbde14147c"
      },
      "source": [
        "print(\"Checking for missing values:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values:\n",
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          0\n",
            "thal        0\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ee8d36d"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to check for and remove duplicate rows in the DataFrame using `df.drop_duplicates(inplace=True)` to ensure data cleanliness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f5469fb",
        "outputId": "f4c20680-90df-46e4-853a-21f4691bcc29"
      },
      "source": [
        "initial_rows = df.shape[0]\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Removed {initial_rows - df.shape[0]} duplicate rows.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 1 duplicate rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d515cfc0"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to display the number of rows and columns after cleaning using `df.shape` to confirm the final dimensions of the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58a4a388",
        "outputId": "bf5694ce-35e0-4e2f-f779-03006b2123c2"
      },
      "source": [
        "print(f\"DataFrame dimensions after cleaning: {df.shape[0]} rows, {df.shape[1]} columns.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame dimensions after cleaning: 302 rows, 14 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afe5e892"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Subtask:\n",
        "Normalize or scale numerical features such as 'Age', 'Cholesterol', and 'Blood Pressure' to improve the performance of the logistic regression model. This will likely involve using a scaler from `sklearn.preprocessing`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61b78fad"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to identify the numerical features to be scaled, import the `StandardScaler`, instantiate it, and then apply it to the identified columns in the DataFrame as specified in the instructions. This will normalize the numerical features to improve model performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6069ea7b",
        "outputId": "683544ed-bb75-4fc8-c40a-49b468d0e008"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical features to scale\n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply scaler to numerical features and update the DataFrame\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "print(\"Numerical features scaled successfully.\")\n",
        "print(df.head())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical features scaled successfully.\n",
            "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
            "0  0.949794    1   3  0.764066 -0.261285    1        0  0.018826      0   \n",
            "1 -1.928548    1   2 -0.091401  0.067741    0        1  1.636979      0   \n",
            "2 -1.485726    0   1 -0.091401 -0.822564    0        0  0.980971      0   \n",
            "3  0.174856    1   1 -0.661712 -0.203222    0        1  1.243374      0   \n",
            "4  0.285561    0   0 -0.661712  2.080602    0        1  0.587366      1   \n",
            "\n",
            "    oldpeak  slope  ca  thal  target  \n",
            "0  1.084022      0   0     1       1  \n",
            "1  2.118926      0   0     2       1  \n",
            "2  0.307844      2   0     2       1  \n",
            "3 -0.209608      2   0     2       1  \n",
            "4 -0.382092      2   0     2       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9588a42"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed data into training and testing sets. Train a Logistic Regression model from `sklearn.linear_model` to classify patients as having heart disease or not based on the selected features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ddccca"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to separate the features (independent variables) from the target variable ('target'). I will assign all columns except 'target' to `X` and the 'target' column to `y`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "308e732f",
        "outputId": "e0f012d6-e4e4-4e52-9eb2-a8b45ee115de"
      },
      "source": [
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "print(\"Features (X) and target (y) separated successfully.\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features (X) and target (y) separated successfully.\n",
            "X shape: (302, 13)\n",
            "y shape: (302,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d98641d"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to import the `train_test_split` function from `sklearn.model_selection`, which is essential for splitting the dataset into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36468308",
        "outputId": "3dec87da-a096-4246-8b3a-bdd03995784a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "print(\"train_test_split imported successfully.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_test_split imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e4019e"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to split the `X` and `y` data into training and testing sets using the `train_test_split` function, with a `test_size` of 0.3 and a `random_state` of 42 for reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5475c345",
        "outputId": "34514361-4f6f-4854-9103-98a49993622c"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(\"Data split into training and testing sets successfully.\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and testing sets successfully.\n",
            "X_train shape: (211, 13)\n",
            "X_test shape: (91, 13)\n",
            "y_train shape: (211,)\n",
            "y_test shape: (91,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8b40f4"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to import the `LogisticRegression` class from `sklearn.linear_model`, which is necessary to build the logistic regression model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df18dc74",
        "outputId": "2a262540-13f4-4c0e-c7e8-7a575993351b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "print(\"LogisticRegression imported successfully.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5745a0"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to instantiate a `LogisticRegression` model with `max_iter=1000` and then train it using the `X_train` and `y_train` data, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc2bc63f",
        "outputId": "9458a6e2-b282-4267-d6df-5de6f10f2017"
      },
      "source": [
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Logistic Regression model instantiated and trained successfully.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model instantiated and trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "574bd680"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Assess the model's accuracy on the test set. Calculate and display a confusion matrix, along with key classification metrics such as precision, recall, and F1-score, using `sklearn.metrics`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fee130c"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in evaluating the model is to make predictions on the test set (`X_test`) using the trained logistic regression model (`model`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7080a17",
        "outputId": "396c4160-36a5-4796-a255-6eee2edc16d8"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(\"Predictions on the test set generated successfully.\")\n",
        "print(y_pred[:5])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions on the test set generated successfully.\n",
            "[0 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "285b3747"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to import the necessary metrics functions (`accuracy_score`, `confusion_matrix`, and `classification_report`) from `sklearn.metrics` to evaluate the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6578dc",
        "outputId": "ee1bef38-4745-43d8-ee76-3d9c3bf32d62"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"Evaluation metrics imported successfully.\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9a93624"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to calculate and print the accuracy of the model using `accuracy_score` with `y_test` and the generated predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3325e663",
        "outputId": "76868f84-5d7e-4fc8-e69f-40c9a6e9ec7b"
      },
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159361c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to calculate and print the confusion matrix using `confusion_matrix` with `y_test` and the generated predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94383a8d",
        "outputId": "375d2f5b-557f-4e27-f8c3-67720e9bff0b"
      },
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[33  9]\n",
            " [ 7 42]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "057cea6d"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to generate and print the classification report, which includes precision, recall, and F1-score, using `classification_report` with `y_test` and the generated predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56740a1b",
        "outputId": "306e0b33-4ce7-4968-87df-7cc3d886b8a6"
      },
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.80        42\n",
            "           1       0.82      0.86      0.84        49\n",
            "\n",
            "    accuracy                           0.82        91\n",
            "   macro avg       0.82      0.82      0.82        91\n",
            "weighted avg       0.82      0.82      0.82        91\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f0d8d36"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from the model evaluation, including the model's performance metrics and any key insights gained from the heart disease prediction analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91fde9ed"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Preparation:** The `heart.csv` dataset, initially containing 303 entries and 14 columns, was loaded and cleaned. One duplicate row was removed, resulting in a cleaned dataset of 302 entries. No missing values were found.\n",
        "*   **Feature Scaling:** Five numerical features ('age', 'trestbps', 'chol', 'thalach', 'oldpeak') were successfully scaled using `StandardScaler` to normalize their ranges.\n",
        "*   **Data Splitting:** The preprocessed data was split into training and testing sets, with 211 samples for training and 91 samples for testing (approximately a 70/30 split).\n",
        "*   **Model Performance (Logistic Regression):**\n",
        "    *   The Logistic Regression model achieved an **accuracy of 0.8242 (82.42\\%)** on the test set.\n",
        "    *   **Confusion Matrix:**\n",
        "        *   True Negatives: 33 (correctly predicted no heart disease)\n",
        "        *   False Positives: 9 (incorrectly predicted heart disease)\n",
        "        *   False Negatives: 7 (incorrectly predicted no heart disease)\n",
        "        *   True Positives: 42 (correctly predicted heart disease)\n",
        "    *   **Classification Report:**\n",
        "        *   For predicting **no heart disease (class 0)**: Precision was 0.82, Recall was 0.79, and F1-score was 0.80.\n",
        "        *   For predicting **heart disease (class 1)**: Precision was 0.82, Recall was 0.86, and F1-score was 0.84.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model demonstrates a reasonably good ability to predict heart disease, with slightly better recall for the positive class (heart disease), indicating it is effective at identifying actual cases.\n",
        "*   To further enhance the model, particularly in reducing false negatives (missed cases of heart disease), consider experimenting with different feature engineering techniques, exploring other machine learning algorithms, or fine-tuning the current model's hyperparameters.\n"
      ]
    }
  ]
}